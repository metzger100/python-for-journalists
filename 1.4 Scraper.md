# REST-API-Scraper

REST-APIs sind APIs welche dynamisch Inhalte zurückgeben, je nach URL.

## REST-APIs finden

Man kann REST-APIs mit dem Entwicklertool des Browsers finden. Im Netzwerk-Reiter taucht beim Laden der Website unter "Fetch/XHR" die interessanten Requests angezeigt. 

Hat man den richtigen Request gefunden kopiert man die cURL und wandelt diese um.

Hilfreich ist hier [curlconverter](https://curlconverter.com/). Hier kann man die kopierte cURL einfügen.

## REST-APIs nutzen

### GET-Request

REST-APIs sind APIs welche dynamisch Inhalte zurückgeben, je nach URL.
In Python geht das mit der *requests*-Library.

```python
response = requests.get("https://rest.api.de/get_json?number=2&multiplier=3")
```

Um die Antwort der API auslesen zu können müssen wir die json-Methode verwenden. 

```python
response.json()
```

Es gibt auch Fälle in den keine JSON-Antwort geliefert wird. Hierfür gibt es die content-Methode:

```python
response.content
```

Um den Typ des Contents zu erkennen können wir die type-Methode verwenden. 

```python
type(response.content)
```

Dabei kann es nötig sein. Die Antwort zu decoden. Dies können wir mit der decode-Methode tun. utf-8 ist die häufigste Kodierung. Bei falschen Umlauten ist die Kodierung meist iso-8895-1

```python
response.content.decode("utf-8")
```

### POST-Request

Wir können auch einen Auftrag an die API senden. Die API erwartet dabei jedoch einen sogenannte Payload, also ein Paket zu verarbeitendem Inhalt.

```python
payload = {"number": 2, "multiplier": 3}

response = requests.post("http://api.marco-lehner.de/post_json", json=payload)
response.json()
```

### Importieren in Pandas

```python
headers = {
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'en-US,en;q=0.9,de;q=0.8',
    'Authorization': 'Basic ZnJvbnRlbmQ6ZnJvbnRlbmQ=',
    'Connection': 'keep-alive',
    'Origin': 'https://xn--strungsauskunft-9sb.de',
    'Referer': 'https://xn--strungsauskunft-9sb.de/',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'cross-site',
    'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
    'sec-ch-ua': '"Not A(Brand";v="24", "Chromium";v="110"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Linux"',
}

params = {
    'SectorType': '1',
}

response = requests.get('https://api-public.stoerungsauskunft.de/api/v1/public/outages', params=params, headers=headers)

content = response.json()

import pandas as pd

stromausfaelle = pd.json_normalize(content)
stromausfaelle
```

So lassen sich entsprechende Datensätze aus einer REST-API importieren.

# Websites direkt scrapen

## Beautiful Soup

Oft sind jedoch keine öffentlichen REST-APIs vorhanden. Dann muss man die Website direkt scrapen. Das geht wunderbar mit der Library Beautiful Soup.

```python 
from bs4 import BeautifulSoup
import requests
```

Dabei muss man sich die hierarchische Architektur eines html-Dokuments genau ansehen um die richtigen Punkte importieren zu können. 

```python
response = requests.get("https://www.zooplus.de/magazin/hund/hunderassen")
response.content.decode("utf-8")
```

Dieses html-Dokument können wir mit BS nun analysieren:

```python
soup = BeautifulSoup(response.content.decode("utf-8"))
```

### find-Methode

Mit der find-Methode lassen sich html-Elemente in einer Datei finden und als wrapper abspeichern. Ein Wrapper ist ein Element welches viele Elemente zusammenfasst.

```python
wrapper = soup.find("section", {"class": "breed-selector__breed-article-wrapper"})
wrapper.contents
```

`breed-selector__breed-selector__breed-article-wrapper` ist der Klassenname des Wrappers. `section` ist das Element der Website.

Um die einzelnen Links zu erhalten können wir diese mit der find_all-Methode leicht finden und in eine Liste schreiben. Die Liste ist ein Dictionary.

```python
links = wrapper.find_all("a", {"class": "breed-selector__breed-article-visible"})
links
```

Oftmals sind in den Links jedoch nicht nur der gewollte Inhalt sondern weitere Elemente wie Bilder, Tags, Beschreibungen oder IDs enthalten. 
Wir erstellen somit also eine Liste und filtern in jedem Fall nach dem ensprechenden Element. Das try-except-Statement schützt uns vor eventuellen Fehlern, falls das gesuchte Element nicht vorhanden ist. 

```python
urls = []

for a in links:
  try:
    urls.append(a["href"])
  except KeyError:
    continue
```

in `urls` stehen nun alle gesuchten weiterführenden Links zu den Seiten mit den Hunderassen.

Diese Links beinhalten nun alles Informationen zu den entsprechenden Hunderassen.

```python
bityness = dict()

for url in urls:
  print(f"Retrieve {url}")

  breed = url.split("/")[-1]
  response = requests.get(url)

  soup = BeautifulSoup(response.content.decode("utf-8"))

  characteristics = soup.find("div", {"class": "breed-selector-article-sidebar--characteristics-wrapper"})
  bite_likeliness = characteristics.find("p", string="Geringe Tendenz zu beißen").parent()[1].find("span", {"class": "sr-only"}).contents[0]
  bityness[breed] = bite_likeliness
  sleep(1)


bityness
```

breed ist der Name der Hunderasse, welchen wir aus der url extrahieren.

## Selenium

Selenium ist ein Open-Source-Framework für das Testen von Webanwendungen und das Automatisieren von Browser-Aktionen. Es ermöglicht es Entwicklern, Webseiten in verschiedenen Browsern zu automatisieren und mit ihnen zu interagieren.
Mit Selenium kann man zum Beispiel folgende Aktionen ausführen:

1. Webseiten öffnen und schließen.
2. Formulare ausfüllen und absenden.
3. Klicken auf Links, Buttons oder andere Elemente.
4. Scrollen und navigieren durch eine Webseite.
5. Screenshots erstellen.
6. Cookies verwalten.
7. JavaScript-Code ausführen.

Selenium unterstützt verschiedene Browser wie Chrome, Firefox, Safari und andere. Es bietet eine API, die es Entwicklern ermöglicht, auf die Funktionen des Frameworks zuzugreifen und Tests oder automatisierte Skripte zu erstellen.
Selenium wird nicht nur für das Testen von Webanwendungen verwendet, sondern auch für andere Anwendungsfälle wie das Web-Scraping, das Durchführen von Web-Interaktionen für Datenextraktion oder das Automatisieren von Aufgaben, die eine Interaktion mit dem Browser erfordern.

Das ist vor allem nützlich wenn die klassischen automatisierten requests geblockt werden. Mit Selenium kann man sich als normalen Nutzer ausgeben.

### Installation

```bash
pip install selenium
wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
apt install ./google-chrome-stable_current_amd64.deb
```

zusätzlich benötigen wir die aktuellste Version des Chrome-Drivers: 

```bash
!wget https://chromedriver.storage.googleapis.com/110.0.5481.77/chromedriver_linux64.zip
!unzip chromedriver_linux64.zip
```

### Setup

```python
from selenium import webdriver

options = webdriver.ChromeOptions()
options.add_argument('--headless')
options.add_argument('--no-sandbox')

browser = webdriver.Chrome(executable_path='chromedriver',options=options)
```

nun können wir mit Selenium die Website laden: 

```python
browser.get("https://k9data.com/")
```

### Methoden

#### title

Gibt das `title`-Attribut zurück. 

```python
browser.title
```

#### save_screeshot

Speichert einen Screenshot der aktuellen Seite.

```python
browser.save_screenshot("screenshot.png")
```

#### find_element

Damit kann man die gesuchten Elemente finden und auswählen.

```python
browser.find_element(By.ID, "id")
browser.find_element(By.NAME, "name")
browser.find_element(By.XPATH, "xpath")
browser.find_element(By.LINK_TEXT, "link text")
browser.find_element(By.PARTIAL_LINK_TEXT, "partial link text")
browser.find_element(By.TAG_NAME, "tag name")
browser.find_element(By.CLASS_NAME, "class name")
browser.find_element(By.CSS_SELECTOR, "css selector")
```

#### send_keys

Mit dieser Methode kann man eine Eingabe machen.
Zum Beispiel kann man damit einen Suchbegriff in ein zuvor mit `find_element` in `input_field` gespeichertes Element eingeben:

```python
input_field.send_keys("Bello")
```

#### click

Mit dieser Methode kann man ein klickbares Element anklicken.

```python
search_button.click()
```

#### WebDriverWait

Mit dieser Klasse können wir Selenium sagen, dass es warten soll, bis die Seite vollständig geladen ist. Dafür können wir ein Element festlegen, auf welches gewartet werden soll.

```python
from selenium.webdriver.support.wait import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

WebDriverWait(browser, 20).until(EC.presence_of_element_located((By.LINK_TEXT, "Click here to return to the home page")))
```

#### page_source

Manche Websites werden erst im Browser gerendert, denn der Webserver liefert nur den entsprechenden JS-Code. Die fertige Seite können wir mit `.page_source` ausgeben lassen um sie mit BeautifulSoup zu analysieren.